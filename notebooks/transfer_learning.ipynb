{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Setze den Seed für reproduzierbare Ergebnisse\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Definiere die Transformationen für die Bilder\n",
    "# Wichtig für Transfer Learning: Größe 224x224 und spezielle Normalisierungswerte\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=256),         # Vergrößere die kleinere Seite auf 256px\n",
    "    transforms.CenterCrop(size=224),     # Schneide einen 224x224 großen Bereich aus der Mitte aus\n",
    "    transforms.ToTensor(),               # Wandle das Bild in einen PyTorch-Tensor um\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Standard-Normalisierung für ImageNet\n",
    "])\n",
    "\n",
    "# Lade den gesamten Datensatz mit den Transformationen\n",
    "full_dataset = datasets.ImageFolder(root='./data/animals', transform=image_transforms)\n",
    "\n",
    "# Teile die Daten in einen Trainings- und einen Testsatz auf\n",
    "# 100 Bilder zum Trainieren, 29 zum Testen\n",
    "train_set, test_set = torch.utils.data.random_split(full_dataset, [100, 29])\n",
    "\n",
    "# Erstelle die DataLoader, um die Daten in Batches zu laden\n",
    "batch_size = 10\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), shuffle=False) # Lade Testdaten in einem Batch\n",
    "\n",
    "# Wähle das Gerät (GPU falls verfügbar, sonst CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    # Der Konstruktor, in dem die einzelnen Schichten des Netzwerks definiert werden.\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Erste Faltungsschicht (Convolutional Layer):\n",
    "        # Nimmt 3-Kanal-Bilder (RGB) auf und erzeugt 10 Feature-Maps mit einem 5x5-Filter.\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5)\n",
    "        \n",
    "        # Pooling-Schicht: Verkleinert die Feature-Maps um den Faktor 2 (downsampling).\n",
    "        # Diese eine Schicht-Definition wird nach beiden Conv-Layern wiederverwendet.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Zweite Faltungsschicht:\n",
    "        # Nimmt die 10 Feature-Maps von conv1 auf und erzeugt 20 neue Feature-Maps.\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        \n",
    "        # Erste vollvernetzte Schicht (Fully Connected Layer):\n",
    "        # Nimmt den flachgedrückten Vektor auf (56180 Features) und reduziert ihn auf 50 Features.\n",
    "        self.fc1 = nn.Linear(in_features=56180, out_features=50)\n",
    "        \n",
    "        # Zweite und letzte Schicht (Output Layer):\n",
    "        # Nimmt die 50 Features auf und gibt die finalen Scores für die 2 Klassen aus.\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "    # Die forward-Methode definiert den Datenfluss durch das zuvor definierte Netzwerk.\n",
    "    # Input x hat die Form [batch_size, 3, 224, 224]\n",
    "    def forward(self, x):\n",
    "        # 1. Stufe: Faltung -> Aktivierung -> Pooling\n",
    "        # Input: [3, 224, 224] -> nach conv1: [10, 220, 220] -> nach pool: [10, 110, 110]\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # 2. Stufe: Faltung -> Aktivierung -> Pooling\n",
    "        # Input: [10, 110, 110] -> nach conv2: [20, 106, 106] -> nach pool: [20, 53, 53]\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # \"Flatten\": Wandelt die 3D-Feature-Map in einen 1D-Vektor um.\n",
    "        # Die -1 sorgt dafür, dass die Batch-Größe automatisch beibehalten wird.\n",
    "        # Input: [batch_size, 20, 53, 53] -> Output: [batch_size, 56180]\n",
    "        x = x.view(-1, 56180) \n",
    "        \n",
    "        # Datenfluss durch die erste vollvernetzte Schicht mit ReLU-Aktivierung.\n",
    "        # Input: [batch_size, 56180] -> Output: [batch_size, 50]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Datenfluss durch die finale Ausgabeschicht.\n",
    "        # Input: [batch_size, 50] -> Output: [batch_size, 2]\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Gibt die finalen Scores (Logits) zurück.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktion und eine Beispielfunktion zum Evaluieren\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(dataloader.dataset)\n",
    "    print(f'Genauigkeit: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(\"\\nTraining abgeschlossen. Evaluiere auf dem Test-Set:\")\n",
    "    evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training des CNN von Grund auf ---\n",
      "Epoch 1/20 - Loss: 0.5391\n",
      "Epoch 2/20 - Loss: 0.4632\n",
      "Epoch 3/20 - Loss: 0.0745\n",
      "Epoch 4/20 - Loss: 0.0954\n",
      "Epoch 5/20 - Loss: 0.2030\n",
      "Epoch 6/20 - Loss: 0.0947\n",
      "Epoch 7/20 - Loss: 0.0375\n",
      "Epoch 8/20 - Loss: 0.2142\n",
      "Epoch 9/20 - Loss: 0.4365\n",
      "Epoch 10/20 - Loss: 0.0303\n",
      "Epoch 11/20 - Loss: 0.0178\n",
      "Epoch 12/20 - Loss: 0.0341\n",
      "Epoch 13/20 - Loss: 0.0132\n",
      "Epoch 14/20 - Loss: 0.0115\n",
      "Epoch 15/20 - Loss: 0.0140\n",
      "Epoch 16/20 - Loss: 0.0001\n",
      "Epoch 17/20 - Loss: 0.0094\n",
      "Epoch 18/20 - Loss: 0.0023\n",
      "Epoch 19/20 - Loss: 0.0036\n",
      "Epoch 20/20 - Loss: 0.0001\n",
      "\n",
      "Training abgeschlossen. Evaluiere auf dem Test-Set:\n",
      "Genauigkeit: 82.76%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training des CNN von Grund auf ---\")\n",
    "cnn_from_scratch = CNN()\n",
    "train(cnn_from_scratch, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/datata1/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Schritt 1: Ein vor-trainiertes Experten-Modell laden\n",
    "# Hier wird die ResNet18-Architektur geladen UND mit den Gewichten initialisiert,\n",
    "# die durch das Training auf dem riesigen ImageNet-Datensatz (Millionen Bilder) gelernt wurden.\n",
    "# Das Modell ist also bereits ein Experte für die Erkennung allgemeiner Bildmerkmale.\n",
    "model_resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Schritt 2: Alle gelernten Gewichte \"einfrieren\"\n",
    "# Diese Schleife geht durch jeden einzelnen Parameter (Gewichte und Biases) im gesamten Modell...\n",
    "for param in model_resnet.parameters():\n",
    "    # ...und teilt PyTorch mit, dass für diesen Parameter keine Gradienten berechnet werden sollen.\n",
    "    # Das bedeutet, die vor-trainierten Gewichte werden beim neuen Training nicht verändert.\n",
    "    # Wir nutzen das Modell als festen \"Feature Extractor\".\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Schritt 3: Die letzte Schicht für unsere Aufgabe anpassen\n",
    "# Die letzte Schicht des originalen ResNet18 war dafür da, 1000 ImageNet-Klassen zu erkennen.\n",
    "# Wir müssen sie durch eine neue Schicht ersetzen, die nur unsere 2 Klassen kennt.\n",
    "\n",
    "# Zuerst lesen wir aus, wie viele Eingangs-Merkmale die letzte Schicht (genannt 'fc') erwartet.\n",
    "# Bei ResNet18 sind das 512.\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "\n",
    "# Jetzt überschreiben wir die alte 'fc'-Schicht mit einer komplett neuen Linearen Schicht.\n",
    "# Diese neue Schicht nimmt die 512 Features vom Rest des Netzwerks entgegen und hat\n",
    "# nur noch 2 Ausgänge (einen für jede unserer Klassen, z.B. Delfin und Elefant).\n",
    "# WICHTIG: Die Parameter dieser neuen Schicht haben standardmäßig `requires_grad=True` und werden somit als einzige trainiert.\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training des ResNet18 via Transfer Learning ---\n",
      "Epoch 1/20 - Loss: 0.4835\n",
      "Epoch 2/20 - Loss: 0.1617\n",
      "Epoch 3/20 - Loss: 0.2018\n",
      "Epoch 4/20 - Loss: 0.0738\n",
      "Epoch 5/20 - Loss: 0.0472\n",
      "Epoch 6/20 - Loss: 0.3996\n",
      "Epoch 7/20 - Loss: 0.2189\n",
      "Epoch 8/20 - Loss: 0.0498\n",
      "Epoch 9/20 - Loss: 0.0146\n",
      "Epoch 10/20 - Loss: 0.0469\n",
      "Epoch 11/20 - Loss: 0.1776\n",
      "Epoch 12/20 - Loss: 0.0304\n",
      "Epoch 13/20 - Loss: 0.0252\n",
      "Epoch 14/20 - Loss: 0.0216\n",
      "Epoch 15/20 - Loss: 0.1558\n",
      "Epoch 16/20 - Loss: 0.0574\n",
      "Epoch 17/20 - Loss: 0.1199\n",
      "Epoch 18/20 - Loss: 0.0384\n",
      "Epoch 19/20 - Loss: 0.2732\n",
      "Epoch 20/20 - Loss: 0.0521\n",
      "\n",
      "Training abgeschlossen. Evaluiere auf dem Test-Set:\n",
      "Genauigkeit: 96.55%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training des ResNet18 via Transfer Learning ---\")\n",
    "# Das Training dauert nur einen Moment, da nur eine Schicht trainiert wird.\n",
    "train(model_resnet, train_loader, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2-vorbereitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
