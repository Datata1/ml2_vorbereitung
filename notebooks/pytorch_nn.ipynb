{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende cuda als Ger√§t\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# √úberpr√ºfen, ob eine GPU verf√ºgbar ist, ansonsten die CPU verwenden\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Verwende {device} als Ger√§t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ist verf√ºgbar! üéâ\n",
      "Anzahl der GPUs: 1\n",
      "Name der GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA ist verf√ºgbar! üéâ\")\n",
    "    print(f\"Anzahl der GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Name der GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA ist nicht verf√ºgbar. √úberpr√ºfe deine Treiber und die PyTorch-Installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form von X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Form von y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Trainingsdaten herunterladen\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # Umwandlung der Bilder in Tensoren\n",
    ")\n",
    "\n",
    "# Testdaten herunterladen\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# DataLoader erstellen\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# √úberpr√ºfen der Datenform\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Form von X [N, C, H, W]: {X.shape}\") # N=Batch Size, C=Channels, H=Height, W=Width\n",
    "    print(f\"Form von y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Definiert eine neue Klasse namens \"NeuralNetwork\".\n",
    "# Sie erbt von \"nn.Module\", der Basisklasse f√ºr alle neuronalen Netz-Module in PyTorch.\n",
    "# Das bedeutet, unsere Klasse bekommt alle Funktionalit√§ten von PyTorch, z.B. das Verwalten von Parametern (Gewichten) und die F√§higkeit, auf einer GPU zu laufen.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    # Der Konstruktor der Klasse. Diese Methode wird aufgerufen, wenn ein neues Objekt\n",
    "    # dieser Klasse erstellt wird (z.B. mit model = NeuralNetwork()).\n",
    "    # Hier werden die Bausteine (Layer) des Netzes definiert und initialisiert.\n",
    "    def __init__(self):\n",
    "        # Ruft den Konstruktor der √ºbergeordneten Klasse (nn.Module) auf.\n",
    "        # Dies ist ein notwendiger Schritt, um sicherzustellen, dass alles korrekt\n",
    "        # initialisiert wird, damit PyTorch die Layer verwalten kann.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Erstellt eine \"Flatten\"-Schicht. Diese Schicht hat eine einfache Aufgabe:\n",
    "        # Sie wandelt einen mehrdimensionalen Tensor in einen eindimensionalen Vektor um.\n",
    "        # Ein Bild der Gr√∂√üe 28x28 Pixel wird also zu einem Vektor der L√§nge 784 (28*28).\n",
    "        # Dies ist notwendig, um die Bilddaten in die nachfolgenden, vollvernetzten (Linear) Schichten zu geben.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # \"nn.Sequential\" ist ein Container, der mehrere Schichten zu einem einzigen\n",
    "        # Block zusammenfasst. Die Daten durchlaufen die Schichten in genau der Reihenfolge,\n",
    "        # in der sie hier definiert sind. Das macht den Code im \"forward\"-Teil √ºbersichtlicher.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # 1. Schicht (Input Layer): Eine vollvernetzte (\"Linear\") Schicht.\n",
    "            # Sie erwartet einen Input mit 784 Features (vom geflatteten 28x28 Bild)\n",
    "            # und transformiert diese linear (y = Wx + b) auf 128 Features.\n",
    "            nn.Linear(28*28, 128),\n",
    "            \n",
    "            # 2. Schicht (Aktivierungsfunktion): Die ReLU (Rectified Linear Unit) Funktion.\n",
    "            # Sie f√ºhrt Nicht-Linearit√§t in das Modell ein, was entscheidend ist,\n",
    "            # um komplexe Muster zu lernen. Sie setzt alle negativen Werte auf 0 und l√§sst\n",
    "            # positive Werte unver√§ndert.\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 3. Schicht (Hidden Layer): Eine weitere vollvernetzte Schicht.\n",
    "            # Sie nimmt die 128 Features von der vorherigen Schicht und gibt wieder 128 Features aus.\n",
    "            nn.Linear(128, 128),\n",
    "            \n",
    "            # 4. Schicht (Aktivierungsfunktion): Erneut eine ReLU-Aktivierung.\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 5. Schicht (Output Layer): Die letzte vollvernetzte Schicht.\n",
    "            # Sie nimmt die 128 Features der letzten versteckten Schicht und erzeugt 10 Ausgabewerte.\n",
    "            # Diese 10 Werte werden als \"Logits\" bezeichnet und repr√§sentieren die Roh-Scores\n",
    "            # f√ºr jede der 10 m√∂glichen Klassen (z.B. Ziffern 0-9).\n",
    "            nn.Linear(128, 10)     \n",
    "        )\n",
    "\n",
    "    # Die \"forward\"-Methode definiert den eigentlichen \"Vorw√§rtsdurchlauf\" der Daten.\n",
    "    # Wenn Daten an das Modell √ºbergeben werden (z.B. model(input_data)), wird diese Methode ausgef√ºhrt.\n",
    "    # 'x' ist hier der Input-Tensor, der die Daten enth√§lt (z.B. ein Batch von Bildern).\n",
    "    def forward(self, x):\n",
    "        # Zuerst wird der Input 'x' durch die Flatten-Schicht geschickt, um ihn zu einem Vektor zu machen.\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Der geflattete Vektor 'x' wird dann durch den gesamten \"linear_relu_stack\" geleitet.\n",
    "        # Das Ergebnis sind die finalen Logits.\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        \n",
    "        # Die Methode gibt die berechneten Logits zur√ºck.\n",
    "        # Diese k√∂nnen sp√§ter mit einer Softmax-Funktion in Wahrscheinlichkeiten umgerechnet werden.\n",
    "        return logits\n",
    "\n",
    "# Erstellt eine Instanz unseres eben definierten neuronalen Netzes.\n",
    "# \".to(device)\" verschiebt das gesamte Modell, inklusive aller seiner Parameter (Gewichte und Biases),\n",
    "# auf das angegebene Rechenger√§t 'device' (typischerweise 'cpu' oder 'cuda' f√ºr eine NVIDIA GPU).\n",
    "# Die Variable 'device' muss zuvor definiert worden sein.\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Gibt eine textuelle Zusammenfassung der Modellarchitektur aus.\n",
    "# Dies zeigt alle in `__init__` definierten Module und Schichten in ihrer Struktur an.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier wird die Verlustfunktion (Loss Function) initialisiert.\n",
    "# nn.CrossEntropyLoss ist eine sehr gebr√§uchliche Verlustfunktion f√ºr Klassifikationsprobleme mit mehreren Klassen.\n",
    "# Unter der Haube kombiniert sie zwei Schritte:\n",
    "# 1. Eine LogSoftmax-Funktion, die die rohen Logits des Modells in logarithmische Wahrscheinlichkeiten umwandelt.\n",
    "# 2. Die Negative Log Likelihood Loss (NLLLoss), die misst, wie gut das Modell die korrekte Klasse vorhersagt.\n",
    "# Die Kombination in einer Klasse ist numerisch stabiler und effizienter als die separate Ausf√ºhrung.\n",
    "# Das Ziel des Trainings wird es sein, den von dieser Funktion berechneten Wert zu minimieren.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definiert die Lernrate (Learning Rate). Dies ist ein sogenannter Hyperparameter,\n",
    "# der steuert, wie stark die Gewichte des Modells bei jedem Trainingsschritt angepasst werden.\n",
    "# \"1e-3\" ist die wissenschaftliche Schreibweise f√ºr 0.001.\n",
    "# Eine zu hohe Lernrate kann dazu f√ºhren, dass das Training instabil wird; eine zu niedrige\n",
    "# Lernrate kann das Training extrem verlangsamen.\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Hier wird der Optimierungsalgorithmus (Optimizer) erstellt.\n",
    "# Der Optimizer hat die Aufgabe, die Gewichte des Modells anzupassen, um den Verlust (loss) zu reduzieren.\n",
    "# torch.optim.Adam: Wir w√§hlen den Adam-Optimizer, einen sehr popul√§ren und oft effektiven\n",
    "# Algorithmus. Adam passt die Lernrate f√ºr jeden einzelnen Parameter des Modells adaptiv an,\n",
    "# was oft zu schnellerer Konvergenz f√ºhrt als bei einfacheren Methoden wie SGD.\n",
    "#\n",
    "# model.parameters(): Dieser Aufruf √ºbergibt dem Optimizer eine Liste aller lernbaren Parameter\n",
    "# (also aller Gewichte und Biases) unseres Modells. Der Optimizer wei√ü somit, welche Werte er anpassen muss.\n",
    "#\n",
    "# lr=learning_rate: Hiermit wird die anf√§ngliche Lernrate f√ºr den Optimizer auf den Wert gesetzt,\n",
    "# den wir zuvor definiert haben.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - dataloader: Ein PyTorch DataLoader, der die Trainingsdaten in Batches bereitstellt.\n",
    "# - model: Das neuronale Netz, das trainiert werden soll.\n",
    "# - loss_fn: Die Verlustfunktion, um den Fehler des Modells zu berechnen.\n",
    "# - optimizer: Der Optimierungsalgorithmus, der die Gewichte des Modells anpasst.\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Ermittelt die Gesamtgr√∂√üe des Datensatzes f√ºr die Fortschrittsanzeige.\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # Versetzt das Modell in den \"Trainingsmodus\".\n",
    "    # Das ist wichtig, da einige Schichten (wie Dropout oder BatchNorm) sich im\n",
    "    # Trainings- und im Auswertungsmodus unterschiedlich verhalten.\n",
    "    model.train()\n",
    "    \n",
    "    # Die Hauptschleife, die √ºber alle Daten-Batches des DataLoaders iteriert.\n",
    "    # \"enumerate\" liefert zus√§tzlich einen Z√§hler (\"batch\") f√ºr den aktuellen Batch.\n",
    "    # Jeder Batch enth√§lt die Eingabedaten 'X' (Bilder) und die zugeh√∂rigen Labels 'y' (korrekte Ziffern).\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Verschiebt die Daten des aktuellen Batches (Bilder und Labels) auf das\n",
    "        # ausgew√§hlte Rechenger√§t ('device', also CPU oder GPU).\n",
    "        # Dies ist notwendig, damit die Berechnungen auf der GPU stattfinden k√∂nnen, falls verf√ºgbar.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # === Schritt 1: Forward Pass ===\n",
    "        # Die Eingabedaten 'X' werden durch das Modell geschickt. Das Modell berechnet\n",
    "        # f√ºr jeden Input die Vorhersagen (Logits).\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Die Verlustfunktion vergleicht die Vorhersagen des Modells ('pred') mit den\n",
    "        # wahren Labels ('y') und berechnet den Fehler (Loss) f√ºr diesen Batch.\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # === Schritt 2: Backward Pass (Backpropagation) ===\n",
    "        # PyTorch berechnet automatisch die Gradienten (Ableitungen) des Verlusts\n",
    "        # in Bezug auf alle lernbaren Parameter (Gewichte) des Modells.\n",
    "        # Diese Gradienten geben an, wie stark jeder Parameter zum Fehler beigetragen hat.\n",
    "        loss.backward()\n",
    "        \n",
    "        # === Schritt 3: Optimierung ===\n",
    "        # Der Optimizer aktualisiert die Gewichte des Modells. Er verwendet die im\n",
    "        # .backward()-Schritt berechneten Gradienten und seine Update-Regel (z.B. Adam),\n",
    "        # um die Gewichte leicht zu ver√§ndern und so den Verlust zu minimieren.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Die berechneten Gradienten m√ºssen nach jedem Update-Schritt zur√ºckgesetzt werden.\n",
    "        # PyTorch akkumuliert Gradienten standardm√§√üig. Ohne .zero_grad() w√ºrden sich\n",
    "        # die Gradienten von Batch zu Batch aufsummieren, was zu falschen Updates f√ºhren w√ºrde.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Dieser Block dient nur zur Ausgabe des Fortschritts alle 100 Batches.\n",
    "        if batch % 100 == 0:\n",
    "            # .item() extrahiert den reinen Python-Zahlenwert aus dem Loss-Tensor.\n",
    "            loss_value = loss.item()\n",
    "            # Berechnet, wie viele Datenpunkte bisher in dieser Epoche verarbeitet wurden.\n",
    "            current = (batch + 1) * len(X)\n",
    "            # Gibt den aktuellen Verlust und den Fortschritt aus.\n",
    "            print(f\"Verlust: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiert die Funktion zur Evaluierung des Modells auf einem Test- oder Validierungsdatensatz.\n",
    "# Sie ben√∂tigt den entsprechenden DataLoader, das Modell und die Verlustfunktion.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    # Ermittelt die Gesamtgr√∂√üe des Datensatzes und die Anzahl der Batches.\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Versetzt das Modell in den \"Evaluierungsmodus\".\n",
    "    # Dies ist das Gegenst√ºck zu model.train() und stellt sicher, dass Schichten\n",
    "    # wie Dropout deaktiviert werden, um konsistente Ergebnisse zu erhalten.\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialisiert Variablen, um den Gesamtverlust und die Anzahl der korrekten\n",
    "    # Vorhersagen √ºber alle Batches hinweg zu summieren.\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # \"torch.no_grad()\" ist ein Kontextmanager, der die Berechnung von Gradienten deaktiviert.\n",
    "    # Da wir das Modell hier nur testen und nicht trainieren, brauchen wir keine Gradienten.\n",
    "    # Das spart Speicher und beschleunigt die Berechnungen erheblich.\n",
    "    with torch.no_grad():\n",
    "        # Schleife, die √ºber alle Batches des Test-DataLoaders iteriert.\n",
    "        for X, y in dataloader:\n",
    "            # Verschiebt die Daten des aktuellen Batches auf das Rechenger√§t (CPU/GPU).\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # === Forward Pass ===\n",
    "            # Die Eingabedaten 'X' werden durch das Modell geschickt, um die Vorhersagen 'pred' zu erhalten.\n",
    "            pred = model(X)\n",
    "            \n",
    "            # Berechnet den Verlust f√ºr den aktuellen Batch und addiert ihn zum Gesamtverlust.\n",
    "            # .item() extrahiert den reinen Python-Zahlenwert aus dem Tensor.\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Z√§hlt die Anzahl der korrekten Vorhersagen in diesem Batch:\n",
    "            # 1. pred.argmax(1): Findet f√ºr jede Eingabe den Index (die Klasse) mit dem h√∂chsten Vorhersagewert.\n",
    "            # 2. == y: Vergleicht die vorhergesagten Klassen mit den wahren Labels 'y'. Das Ergebnis ist ein Tensor mit True/False-Werten.\n",
    "            # 3. .type(torch.float): Wandelt den True/False-Tensor in einen Tensor mit 1.0/0.0-Werten um.\n",
    "            # 4. .sum(): Summiert alle Einsen auf, um die Anzahl der korrekten Vorhersagen zu erhalten.\n",
    "            # 5. .item(): Extrahiert die Summe als Python-Zahl und addiert sie zur Gesamtsumme.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    # Berechnet den durchschnittlichen Verlust pro Batch.\n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    # Berechnet die Genauigkeit (Accuracy), indem die Gesamtzahl der korrekten Vorhersagen\n",
    "    # durch die Gesamtgr√∂√üe des Datensatzes geteilt wird.\n",
    "    correct /= size\n",
    "    \n",
    "    # Gibt eine formatierte Zusammenfassung der Ergebnisse aus.\n",
    "    print(f\"Test Fehler: \\n Genauigkeit: {(100*correct):>0.1f}%, Avg Verlust: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 1\n",
      "-------------------------------\n",
      "Verlust: 2.303267  [   64/60000]\n",
      "Verlust: 0.339238  [ 6464/60000]\n",
      "Verlust: 0.230606  [12864/60000]\n",
      "Verlust: 0.268099  [19264/60000]\n",
      "Verlust: 0.144082  [25664/60000]\n",
      "Verlust: 0.290033  [32064/60000]\n",
      "Verlust: 0.141095  [38464/60000]\n",
      "Verlust: 0.274460  [44864/60000]\n",
      "Verlust: 0.279492  [51264/60000]\n",
      "Verlust: 0.224199  [57664/60000]\n",
      "Test Fehler: \n",
      " Genauigkeit: 94.6%, Avg Verlust: 0.168203 \n",
      "\n",
      "Epoche 2\n",
      "-------------------------------\n",
      "Verlust: 0.121027  [   64/60000]\n",
      "Verlust: 0.098320  [ 6464/60000]\n",
      "Verlust: 0.103754  [12864/60000]\n",
      "Verlust: 0.097643  [19264/60000]\n",
      "Verlust: 0.053172  [25664/60000]\n",
      "Verlust: 0.141850  [32064/60000]\n",
      "Verlust: 0.059070  [38464/60000]\n",
      "Verlust: 0.191124  [44864/60000]\n",
      "Verlust: 0.184738  [51264/60000]\n",
      "Verlust: 0.147611  [57664/60000]\n",
      "Test Fehler: \n",
      " Genauigkeit: 96.8%, Avg Verlust: 0.105084 \n",
      "\n",
      "Epoche 3\n",
      "-------------------------------\n",
      "Verlust: 0.051860  [   64/60000]\n",
      "Verlust: 0.058553  [ 6464/60000]\n",
      "Verlust: 0.080086  [12864/60000]\n",
      "Verlust: 0.101031  [19264/60000]\n",
      "Verlust: 0.061670  [25664/60000]\n",
      "Verlust: 0.035604  [32064/60000]\n",
      "Verlust: 0.050268  [38464/60000]\n",
      "Verlust: 0.143616  [44864/60000]\n",
      "Verlust: 0.156901  [51264/60000]\n",
      "Verlust: 0.138306  [57664/60000]\n",
      "Test Fehler: \n",
      " Genauigkeit: 97.2%, Avg Verlust: 0.090646 \n",
      "\n",
      "Epoche 4\n",
      "-------------------------------\n",
      "Verlust: 0.034437  [   64/60000]\n",
      "Verlust: 0.047650  [ 6464/60000]\n",
      "Verlust: 0.084994  [12864/60000]\n",
      "Verlust: 0.211457  [19264/60000]\n",
      "Verlust: 0.115001  [25664/60000]\n",
      "Verlust: 0.044766  [32064/60000]\n",
      "Verlust: 0.026683  [38464/60000]\n",
      "Verlust: 0.058380  [44864/60000]\n",
      "Verlust: 0.110748  [51264/60000]\n",
      "Verlust: 0.150515  [57664/60000]\n",
      "Test Fehler: \n",
      " Genauigkeit: 96.7%, Avg Verlust: 0.119362 \n",
      "\n",
      "Epoche 5\n",
      "-------------------------------\n",
      "Verlust: 0.039721  [   64/60000]\n",
      "Verlust: 0.051125  [ 6464/60000]\n",
      "Verlust: 0.027801  [12864/60000]\n",
      "Verlust: 0.140224  [19264/60000]\n",
      "Verlust: 0.118270  [25664/60000]\n",
      "Verlust: 0.068588  [32064/60000]\n",
      "Verlust: 0.011424  [38464/60000]\n",
      "Verlust: 0.048699  [44864/60000]\n",
      "Verlust: 0.084974  [51264/60000]\n",
      "Verlust: 0.052043  [57664/60000]\n",
      "Test Fehler: \n",
      " Genauigkeit: 96.3%, Avg Verlust: 0.149391 \n",
      "\n",
      "Training abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoche {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echtes Label: 7\n",
      "Vorhergesagtes Label: 7\n",
      "Wahrscheinlichkeiten pro Klasse:\n",
      " [1.4358498e-09 1.8572498e-08 4.1451617e-08 3.2412430e-08 3.7418841e-09\n",
      " 6.9738125e-11 2.5839524e-15 9.9999845e-01 3.8575190e-10 1.5233356e-06]\n"
     ]
    }
   ],
   "source": [
    "# Modell wieder in den Eval-Modus versetzen\n",
    "model.eval()\n",
    "\n",
    "# Ein einzelnes Bild aus dem Testdatensatz nehmen\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    logits = model(x)\n",
    "    # Manuell Softmax anwenden, um Wahrscheinlichkeiten zu erhalten\n",
    "    probabilities = nn.functional.softmax(logits, dim=1)\n",
    "    prediction = probabilities.argmax(1)\n",
    "    \n",
    "    print(f\"Echtes Label: {y}\")\n",
    "    print(f\"Vorhergesagtes Label: {prediction.item()}\")\n",
    "    print(f\"Wahrscheinlichkeiten pro Klasse:\\n {probabilities.cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr eine modifizierte Netzwerkarchitektur (nur zur Veranschaulichung)\n",
    "# Sie w√ºrden dies in Zelle 3 einf√ºgen und diese erneut ausf√ºhren.\n",
    "class ModifiedNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 256), # Ge√§ndert auf 256 Neuronen\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),   # Ge√§ndert auf 256 Neuronen\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),   # Neue dritte versteckte Schicht\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# model = ModifiedNeuralNetwork().to(device) # So w√ºrden Sie das neue Modell initialisieren"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2-vorbereitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
